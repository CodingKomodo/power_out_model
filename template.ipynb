{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Title Here\n",
    "\n",
    "**Name(s)**: Ethan Cao\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_times(date_col_name, time_col_name, new_col_name, df):\n",
    "    df = df.copy()\n",
    "    df[new_col_name] = df[date_col_name] + pd.to_timedelta(df[time_col_name].astype(str))\n",
    "    return df\n",
    "\n",
    "data = pd.read_excel(\"outage.xlsx\", skiprows=[0,1,2,3,4,6], index_col=1).iloc[:,1:]\n",
    "data = combine_times(\"OUTAGE.START.DATE\", 'OUTAGE.START.TIME', 'OUTAGE.START.DATETIME', data)\n",
    "data = combine_times(\"OUTAGE.RESTORATION.DATE\", \"OUTAGE.RESTORATION.TIME\", \"OUTAGE.RESTORATION.DATETIME\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['severe weather', 'intentional attack',\n",
       "       'system operability disruption', 'equipment failure',\n",
       "       'public appeal', 'fuel supply emergency', 'islanding'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CAUSE.CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "# Remove rows with missing target variable (OUTAGE.DURATION)\n",
    "# data = data.dropna(subset=['OUTAGE.DURATION'])\n",
    "\n",
    "col_trans = ColumnTransformer([\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore'), ['POSTAL.CODE'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('col-trans', col_trans),\n",
    "    ('lin-reg', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "# Select relevant features and target variable\n",
    "selected_features_and_target = data[['DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED', 'RES.PRICE', 'COM.PRICE', 'IND.PRICE','POSTAL.CODE','OUTAGE.DURATION']].dropna()\n",
    "X = selected_features_and_target.drop(columns=['OUTAGE.DURATION'])\n",
    "y = selected_features_and_target['OUTAGE.DURATION']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Model Training without Feature Engineering\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = pl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13688010761893388"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3672.567230209743"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'columntransformer__num-features__polynomialfeatures__degree': 2}\n",
      "Best Score (Cross-Validation): 4718.595280345606\n"
     ]
    }
   ],
   "source": [
    "col_trans = ColumnTransformer([\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore'), ['POSTAL.CODE']),\n",
    "    ('num-features', make_pipeline(StandardScaler(),PolynomialFeatures()), ['DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED', 'RES.PRICE', 'COM.PRICE', 'IND.PRICE']),\n",
    "    ('time-features', FunctionTransformer(lambda x: pd.DataFrame(x['OUTAGE.START.DATETIME'].apply(lambda y: y.hour))),['OUTAGE.START.DATETIME'] )\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Model Training with Feature Engineering using Pipelines and GridSearchCV\n",
    "model = make_pipeline(\n",
    "    col_trans,\n",
    "    RandomForestRegressor()\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'columntransformer__num-features__polynomialfeatures__degree': [2,3,4,5]\n",
    "}\n",
    "selected_features_and_target = data[['DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED', 'RES.PRICE', 'COM.PRICE', 'IND.PRICE','POSTAL.CODE','OUTAGE.START.DATETIME','OUTAGE.DURATION']].dropna()\n",
    "X = selected_features_and_target.drop(columns=['OUTAGE.DURATION'])\n",
    "y = selected_features_and_target['OUTAGE.DURATION']\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (Cross-Validation):\", np.sqrt(-grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2507572267035598"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_trans = ColumnTransformer([\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore'), ['POSTAL.CODE']),\n",
    "    ('num-features', make_pipeline(StandardScaler(),PolynomialFeatures(degree=2)), ['DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED', 'RES.PRICE', 'COM.PRICE', 'IND.PRICE']),\n",
    "    ('time-features', FunctionTransformer(lambda x: pd.DataFrame(x['OUTAGE.START.DATETIME'].apply(lambda y: y.hour))),['OUTAGE.START.DATETIME'] )\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "\n",
    "\n",
    "# Model Training with Feature Engineering using Pipelines and GridSearchCV\n",
    "model = make_pipeline(\n",
    "    col_trans,\n",
    "    RandomForestRegressor()\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "(model.score(X_test, y_test))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25223583174084446"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.Series(np.array(sorted(scores))) \n",
    "temp = temp[temp > 0]\n",
    "temp.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
